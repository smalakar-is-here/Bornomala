{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b17dbc9",
   "metadata": {
    "papermill": {
     "duration": 0.002339,
     "end_time": "2026-01-15T06:22:15.665994",
     "exception": false,
     "start_time": "2026-01-15T06:22:15.663655",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Project: Bangla Dialect-to-Standard Normalization\n",
    "## Phase 1C (Part 2): Intelligent Audio Segmentation (VAD Pipeline)\n",
    "\n",
    "**Author:** Swagotam Malakar  \n",
    "**Affiliation:** Dept. of CSE, United International University  \n",
    "**Objective:** Transform raw, long-form YouTube audio (30min+) into clean, short (15s) speech segments suitable for ASR training by removing silence, music, and noise.\n",
    "\n",
    "---\n",
    "\n",
    "### Abstract\n",
    "Raw audio from dramas contains significant non-speech artifacts (background music, silence). To prepare the dataset for Phase 2 (Annotation) and Model Fine-tuning, we employ a **Voice Activity Detection (VAD)** pipeline. This notebook utilizes the **Silero VAD** model to precisely identify human speech timestamps and segment the audio into 10-20 second chunks, discarding unusable sections.\n",
    "\n",
    "### Methodology\n",
    "1.  **VAD Model:** Utilization of pre-trained `silero-vad` (Enterprise-grade accuracy).\n",
    "2.  **Timestamp Calculation:** Identification of start/end points of human speech.\n",
    "3.  **Adaptive Chunking:** Merging short speech bursts into training-friendly segments (Target: 15s).\n",
    "4.  **Manifest Generation:** Exporting a CSV ready for annotation tools (Label Studio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad30ac5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T06:22:15.670512Z",
     "iopub.status.busy": "2026-01-15T06:22:15.669969Z",
     "iopub.status.idle": "2026-01-15T06:22:21.182869Z",
     "shell.execute_reply": "2026-01-15T06:22:21.182087Z"
    },
    "papermill": {
     "duration": 5.516986,
     "end_time": "2026-01-15T06:22:21.184471",
     "exception": false,
     "start_time": "2026-01-15T06:22:15.667485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[06:22:21] INFO: Installing Silero VAD & Torchaudio...\n",
      "[06:22:21] INFO: ✓ Dependencies Loaded.\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Dependency Installation\n",
    "# Installing Silero VAD dependencies and Audio Processing tools\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "import torch\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import torchaudio\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s] %(levelname)s: %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(\"Segmenter\")\n",
    "\n",
    "logger.info(\"Installing Silero VAD & Torchaudio...\")\n",
    "# Note: silero-vad is loaded via torch.hub, so internet access is required initially.\n",
    "logger.info(\"✓ Dependencies Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b96f8964",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T06:22:21.188954Z",
     "iopub.status.busy": "2026-01-15T06:22:21.188632Z",
     "iopub.status.idle": "2026-01-15T06:22:21.214752Z",
     "shell.execute_reply": "2026-01-15T06:22:21.214059Z"
    },
    "papermill": {
     "duration": 0.030088,
     "end_time": "2026-01-15T06:22:21.216213",
     "exception": false,
     "start_time": "2026-01-15T06:22:21.186125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[06:22:21] INFO: ✓ Found Input Directory: /kaggle/input/phase-1c-youtube-mining/processed_audio\n",
      "[06:22:21] INFO: ✓ Found 5 raw audio files for processing.\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Pipeline Configuration (AUTO-PATH DETECTION)\n",
    "# Finds where the input files are located automatically.\n",
    "\n",
    "def find_input_directory():\n",
    "    # Standard Kaggle Input Paths to check\n",
    "    potential_paths = [\n",
    "        \"/kaggle/input/phase-1c-youtube-mining/processed_audio\", # Most likely\n",
    "        \"/kaggle/input/phase_1c_youtube_mining/processed_audio\",\n",
    "        \"/kaggle/input/processed_audio\",\n",
    "        \"/kaggle/working/processed_audio\" # If running in same session\n",
    "    ]\n",
    "    \n",
    "    for p in potential_paths:\n",
    "        if os.path.exists(p) and len(glob(f\"{p}/*.wav\")) > 0:\n",
    "            return p\n",
    "            \n",
    "    # Fallback: Search recursively\n",
    "    wavs = glob(\"/kaggle/input/**/*.wav\", recursive=True)\n",
    "    if wavs:\n",
    "        return os.path.dirname(wavs[0])\n",
    "        \n",
    "    return None\n",
    "\n",
    "INPUT_DIR = find_input_directory()\n",
    "\n",
    "SEGMENT_CONFIG = {\n",
    "    \"paths\": {\n",
    "        \"input_dir\": INPUT_DIR, \n",
    "        \"output_dir\": \"/kaggle/working/segmented_dataset\",\n",
    "        \"manifest_path\": \"/kaggle/working/manifests\"\n",
    "    },\n",
    "    \"params\": {\n",
    "        \"sampling_rate\": 16000,\n",
    "        \"min_duration_sec\": 2.0,   # Ignore chunks shorter than 2s\n",
    "        \"max_duration_sec\": 20.0,  # Split if longer than 20s\n",
    "        \"target_duration\": 15.0,   # Ideal chunk size for ASR\n",
    "        \"vad_threshold\": 0.5       # Sensitivity of speech detection\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create Output Directories\n",
    "os.makedirs(SEGMENT_CONFIG['paths']['output_dir'], exist_ok=True)\n",
    "os.makedirs(SEGMENT_CONFIG['paths']['manifest_path'], exist_ok=True)\n",
    "\n",
    "# Check Input Availability\n",
    "if not INPUT_DIR:\n",
    "    logger.critical(\"❌ CRITICAL: Could not find input audio files!\")\n",
    "    logger.critical(\"ACTION: Make sure you added the 'Phase 1C YouTube Mining' output as input to this notebook.\")\n",
    "else:\n",
    "    input_files = glob(f\"{INPUT_DIR}/*.wav\")\n",
    "    logger.info(f\"✓ Found Input Directory: {INPUT_DIR}\")\n",
    "    logger.info(f\"✓ Found {len(input_files)} raw audio files for processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "922d0ab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T06:22:21.220653Z",
     "iopub.status.busy": "2026-01-15T06:22:21.220054Z",
     "iopub.status.idle": "2026-01-15T06:22:22.618223Z",
     "shell.execute_reply": "2026-01-15T06:22:22.617460Z"
    },
    "papermill": {
     "duration": 1.402118,
     "end_time": "2026-01-15T06:22:22.619839",
     "exception": false,
     "start_time": "2026-01-15T06:22:21.217721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[06:22:21] INFO: Loading Silero VAD Model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to /root/.cache/torch/hub/master.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[06:22:22] INFO: ✓ VAD Model Loaded Successfully.\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: VAD Model Loader\n",
    "# Load the Enterprise-grade Silero VAD model from PyTorch Hub\n",
    "\n",
    "def load_vad_model():\n",
    "    logger.info(\"Loading Silero VAD Model...\")\n",
    "    try:\n",
    "        model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', # Repo\n",
    "                                      model='silero_vad',                # Model Name\n",
    "                                      force_reload=False,\n",
    "                                      trust_repo=True)\n",
    "        (get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils\n",
    "        logger.info(\"✓ VAD Model Loaded Successfully.\")\n",
    "        return model, get_speech_timestamps, read_audio, collect_chunks\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load VAD model: {e}\")\n",
    "        raise\n",
    "\n",
    "if INPUT_DIR:\n",
    "    model, get_speech_timestamps, read_audio, collect_chunks = load_vad_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f041d79e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T06:22:22.624830Z",
     "iopub.status.busy": "2026-01-15T06:22:22.624449Z",
     "iopub.status.idle": "2026-01-15T06:22:22.633270Z",
     "shell.execute_reply": "2026-01-15T06:22:22.632688Z"
    },
    "papermill": {
     "duration": 0.012967,
     "end_time": "2026-01-15T06:22:22.634655",
     "exception": false,
     "start_time": "2026-01-15T06:22:22.621688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CELL 4: Intelligent Segmentation Engine\n",
    "# Core logic to split long audio based on speech timestamps\n",
    "\n",
    "def process_audio_file(file_path, config, model_utils):\n",
    "    model, get_timestamps, read_audio, _ = model_utils\n",
    "    params = config['params']\n",
    "    \n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_id = os.path.splitext(file_name)[0]\n",
    "    dialect = file_name.split('_')[0] # Assuming filename format: Dialect_ID.wav\n",
    "    \n",
    "    logger.info(f\"Processing: {file_name}...\")\n",
    "    \n",
    "    # Read Audio\n",
    "    wav = read_audio(file_path, sampling_rate=params['sampling_rate'])\n",
    "    \n",
    "    # Get Speech Timestamps (The Magic Step)\n",
    "    # This detects where speech is and ignores music/silence\n",
    "    speech_timestamps = get_timestamps(\n",
    "        wav, \n",
    "        model, \n",
    "        sampling_rate=params['sampling_rate'],\n",
    "        threshold=params['vad_threshold']\n",
    "    )\n",
    "    \n",
    "    segments_meta = []\n",
    "    chunk_id = 0\n",
    "    \n",
    "    # Logic to merge short timestamps into target_duration (e.g., 15s)\n",
    "    current_chunk = []\n",
    "    current_duration = 0\n",
    "    \n",
    "    for ts in speech_timestamps:\n",
    "        duration = (ts['end'] - ts['start']) / params['sampling_rate']\n",
    "        \n",
    "        # If adding this speech part exceeds max duration, save current chunk first\n",
    "        if current_duration + duration > params['max_duration_sec']:\n",
    "            if current_duration >= params['min_duration_sec']:\n",
    "                # Save Logic\n",
    "                save_chunk(wav, current_chunk, chunk_id, file_id, dialect, config, segments_meta)\n",
    "                chunk_id += 1\n",
    "            \n",
    "            # Reset\n",
    "            current_chunk = []\n",
    "            current_duration = 0\n",
    "        \n",
    "        current_chunk.append(ts)\n",
    "        current_duration += duration\n",
    "    \n",
    "    # Save remaining\n",
    "    if current_duration >= params['min_duration_sec']:\n",
    "        save_chunk(wav, current_chunk, chunk_id, file_id, dialect, config, segments_meta)\n",
    "        \n",
    "    logger.info(f\"   -> Generated {chunk_id + 1} clean segments.\")\n",
    "    return segments_meta\n",
    "\n",
    "def save_chunk(wav_tensor, timestamps, chunk_id, parent_id, dialect, config, meta_list):\n",
    "    # Combine timestamps to a single tensor\n",
    "    # Note: simple concatenation of active speech frames\n",
    "    chunk_audio = torch.cat([wav_tensor[ts['start']:ts['end']] for ts in timestamps])\n",
    "    \n",
    "    # Filename: Dialect_ParentID_Chunk001.wav\n",
    "    out_name = f\"{parent_id}_seg{chunk_id:04d}.wav\"\n",
    "    out_path = os.path.join(config['paths']['output_dir'], out_name)\n",
    "    \n",
    "    # Save\n",
    "    torchaudio.save(out_path, chunk_audio.unsqueeze(0), config['params']['sampling_rate'])\n",
    "    \n",
    "    # Metadata\n",
    "    meta_list.append({\n",
    "        \"filename\": out_name,\n",
    "        \"filepath\": out_path,\n",
    "        \"dialect\": dialect,\n",
    "        \"source_file\": parent_id,\n",
    "        \"duration\": len(chunk_audio) / config['params']['sampling_rate']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84fea033",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T06:22:22.639129Z",
     "iopub.status.busy": "2026-01-15T06:22:22.638688Z",
     "iopub.status.idle": "2026-01-15T06:26:11.803554Z",
     "shell.execute_reply": "2026-01-15T06:26:11.802634Z"
    },
    "papermill": {
     "duration": 229.168576,
     "end_time": "2026-01-15T06:26:11.804969",
     "exception": false,
     "start_time": "2026-01-15T06:22:22.636393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[06:22:22] INFO: Starting Segmentation Pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0c66278bca4edd9a428a468119385d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[06:22:22] INFO: Processing: Chittagonian_JvwgOr-K0vQ.wav...\n",
      "[06:22:59] INFO:    -> Generated 73 clean segments.\n",
      "[06:22:59] INFO: Processing: Noakhali_wMP0zweZUzA.wav...\n",
      "[06:24:02] INFO:    -> Generated 154 clean segments.\n",
      "[06:24:02] INFO: Processing: Chittagonian_mumxd18fIK0.wav...\n",
      "[06:25:03] INFO:    -> Generated 143 clean segments.\n",
      "[06:25:03] INFO: Processing: Sylheti_6Ycv4OO9kwo.wav...\n",
      "[06:25:42] INFO:    -> Generated 117 clean segments.\n",
      "[06:25:42] INFO: Processing: Sylheti_B8tTlSZo7Z8.wav...\n",
      "[06:26:11] INFO:    -> Generated 90 clean segments.\n",
      "[06:26:11] INFO: === SEGMENTATION COMPLETE ===\n",
      "[06:26:11] INFO: Total Segments Created: 577\n",
      "[06:26:11] INFO: Total Clean Speech Duration: 2.82 hours\n",
      "[06:26:11] INFO: Manifest Saved: /kaggle/working/manifests/segmented_inventory.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               filename  \\\n",
      "0  Chittagonian_JvwgOr-K0vQ_seg0000.wav   \n",
      "1  Chittagonian_JvwgOr-K0vQ_seg0001.wav   \n",
      "2  Chittagonian_JvwgOr-K0vQ_seg0002.wav   \n",
      "3  Chittagonian_JvwgOr-K0vQ_seg0003.wav   \n",
      "4  Chittagonian_JvwgOr-K0vQ_seg0004.wav   \n",
      "\n",
      "                                            filepath       dialect  \\\n",
      "0  /kaggle/working/segmented_dataset/Chittagonian...  Chittagonian   \n",
      "1  /kaggle/working/segmented_dataset/Chittagonian...  Chittagonian   \n",
      "2  /kaggle/working/segmented_dataset/Chittagonian...  Chittagonian   \n",
      "3  /kaggle/working/segmented_dataset/Chittagonian...  Chittagonian   \n",
      "4  /kaggle/working/segmented_dataset/Chittagonian...  Chittagonian   \n",
      "\n",
      "                source_file  duration  \n",
      "0  Chittagonian_JvwgOr-K0vQ    18.612  \n",
      "1  Chittagonian_JvwgOr-K0vQ    19.088  \n",
      "2  Chittagonian_JvwgOr-K0vQ     5.820  \n",
      "3  Chittagonian_JvwgOr-K0vQ    19.924  \n",
      "4  Chittagonian_JvwgOr-K0vQ    18.536  \n"
     ]
    }
   ],
   "source": [
    "# CELL 5: Execution Loop & Manifest Generation\n",
    "\n",
    "if INPUT_DIR and input_files:\n",
    "    full_inventory = []\n",
    "    \n",
    "    logger.info(\"Starting Segmentation Pipeline...\")\n",
    "    model_utils = (model, get_speech_timestamps, read_audio, collect_chunks)\n",
    "    \n",
    "    for wav_file in tqdm(input_files):\n",
    "        try:\n",
    "            file_meta = process_audio_file(wav_file, SEGMENT_CONFIG, model_utils)\n",
    "            full_inventory.extend(file_meta)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process {wav_file}: {e}\")\n",
    "            \n",
    "    # Export Inventory\n",
    "    df_seg = pd.DataFrame(full_inventory)\n",
    "    csv_path = f\"{SEGMENT_CONFIG['paths']['manifest_path']}/segmented_inventory.csv\"\n",
    "    df_seg.to_csv(csv_path, index=False)\n",
    "    \n",
    "    logger.info(\"=== SEGMENTATION COMPLETE ===\")\n",
    "    logger.info(f\"Total Segments Created: {len(df_seg)}\")\n",
    "    if not df_seg.empty:\n",
    "        logger.info(f\"Total Clean Speech Duration: {df_seg['duration'].sum() / 3600:.2f} hours\")\n",
    "    logger.info(f\"Manifest Saved: {csv_path}\")\n",
    "    \n",
    "    # Preview\n",
    "    print(df_seg.head())\n",
    "else:\n",
    "    logger.warning(\"Skipping execution as no input files were found.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 291963549,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 240.038409,
   "end_time": "2026-01-15T06:26:13.227249",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-15T06:22:13.188840",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02137ab5515f4043a8fd6d306164ad8f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0e0c66278bca4edd9a428a468119385d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d80ac1cf9f0748d5841634389305c186",
        "IPY_MODEL_1668be3d5a7c4b13b366da3a66b6e389",
        "IPY_MODEL_bed00b523a564f2fbc151bd507977240"
       ],
       "layout": "IPY_MODEL_6dc5f6cca72e40f0b8fffdab20064843",
       "tabbable": null,
       "tooltip": null
      }
     },
     "1668be3d5a7c4b13b366da3a66b6e389": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6b38ea12deb141df85587b4fbc874250",
       "max": 5.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_948aca509d364073b663661bcd5f1284",
       "tabbable": null,
       "tooltip": null,
       "value": 5.0
      }
     },
     "2337bdf09f954f0fbf2947f94aef15d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "279206c9b0f548f69e09977dc9efd5e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6b38ea12deb141df85587b4fbc874250": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6dc5f6cca72e40f0b8fffdab20064843": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "948aca509d364073b663661bcd5f1284": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "becc7eefe56c46278e4964b33a074276": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bed00b523a564f2fbc151bd507977240": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_02137ab5515f4043a8fd6d306164ad8f",
       "placeholder": "​",
       "style": "IPY_MODEL_2337bdf09f954f0fbf2947f94aef15d0",
       "tabbable": null,
       "tooltip": null,
       "value": " 5/5 [03:49&lt;00:00, 42.12s/it]"
      }
     },
     "d80ac1cf9f0748d5841634389305c186": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_becc7eefe56c46278e4964b33a074276",
       "placeholder": "​",
       "style": "IPY_MODEL_279206c9b0f548f69e09977dc9efd5e4",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
